\section{Logical Expressivism}\label{sec:logical_expr}

\subsection{Representationalism v. Expressivism}

Sequent calculi are syntax only. There are multiple ways to interpret
them.

The standard way approach is representational. A prelogical structure
like \(\ulcorner A ; B\urcorner\) is usually taken as representing a
mathematical structure: a set, multiset, sequence, or tree. The
problem with this is that it only goes half way; it does not address
the question of how such a structure might be related to the
non-logical and non-mathematical origins of the structure. So that
representational interpretation must presuppose another layer of
representation, in which the set (for example) represented by
\(\ulcorner A ; B\urcorner\) itself represents something. It also
treats propositions A and B as representations of facts about the
world, so \(\ulcorner A ; B\urcorner\), or rather the set it
represents, must represent a conjunction of two facts. That
conjunction must also be something in the world.

The problem with this should be obvious: there are no pairs in nature.
If we can treat an apple and an orange as a pair, apple \textit{and}
orange, it is not because they are paired in the world, but because it
is useful \textit{to us} to treat them as a pair. With the normative
practices instituting the meaning of \textit{and} in place, we can
infer that a box contains an apple if we are told that it contains and
apple and an orange. If we did not have \textit{and} we would have no
way of doing this; we would have to open the box and see that it
contains an apple. And the justification of ``the box contains an
apple and an orange'' would derive, not from ``seeing'' the pairing of
the two in the world and then representing that pairing by ``and'',
but from the norms of practice, which underwrite the (practical)
transition from two distinct ``seeings'' (of an apple and also an
orange in the box), to the expression ``apple and orange''.

Note that we could (and probably should) explain this as a transition
not from two seeings but from the two separate propositions that arise
from the seeings, i.e. ``the box contains an apple'' and ``the box
contins an orange''. This would be more in line with the
Sellars/Brandom model, where the transition from (sensory) observation
to concept (proposition) is treated as a ``language entrance''
transition.

In any case, the inference to ``the box contains an apple and an
orange'' is also underwritten by normative practices. Since two
propositions are involved, this is most easily illustrated by
considering a scenario involving three actors; this allows us to avoid
the chicken-and-egg problem of going from two things to a pair. One
says ``that box contains an apple'', a second says ``that box contains
an orange'', and the third draws the inference to ``that box contains
an apple and an orange''. In this scenario we entirely avoid the
question of whether pairs ``exist'' in the world.

Which suggests that a pragmatic explanation of the logical connectives
should not be restricted to a scenario involving only one actor.
Better to have one actor per premise. So Brandom's favored example
should be taken to invole two actors, one of whom declares
``Pittsburgh is west of Princeton'', and the other of whom infers ``So
Princeton is east of Pittsburgh''. This is not just a matter of
convenience. An essential feature of Brandom's account is that
rationality is essentially social. It is indeed instituted by norms of
social practice.

This ``social'' strategy allows us to avoid pesky metaphysical
questions about how words like ``and'' and ``or'' relate to
facts-in-the-world. A good pragmatist would not claim that they cannot
represent facts in the world; rather, the pragmatist would argue that
these are the wrong questions. We will probably never be able to give
satisfactory answers to such questions; but we \textit{can} give a
good account of how we use language. And that is enough: the ultimate
ontological status of our sayings is irrelevant. Whether ``and''
corresponds to something in the world has no effect on how we use it.

We can call this a \textit{discursive} strategy (order?) of explanation.

NB: maybe using one actor per premise is in some way essential to the
pragmatist's explanatory strategy. Brandom argues that propositions
and assertions are primitive. Maybe we should argue that a plurality
of rational agents is also a primitive notion. It must be if
rationality is essentially social. So in the ``order of explanation''
plurality of agents comes first. To argue for one agent per premise,
we would need to show that a single agent would not be capable of
managing more than one premise until normative practices for doing so
are jointly instituted by at least two agents.

This would imply that a single agent would be unable to make
inferences from multiple premises in the absence of multi-agent
discursive practices. Indeed would be unable for form the concept ``A
and B''.

Conjecture: connectives can only be jointly instituted. A single
\textit{autonomous} agent would not be able to come up with them. That
seems a little strong. What would the agent have to be able to do in
order to make the inference to A and B good?

Key insight: norms can only be socially instituted. So no, a single
non-social agent would not be able autonomously to come up with
``and'', since it involves normative practice.

Brandom: deontic scorekeeping, discursive commitments, etc. only make
sense with multiple agents, since we keep score of who has what
commitments. The ``game of giving and asking for reasons'' cannot be
solitaire; it only makes sense as a multi-player game.

[This upshot of this is that all the argumentation elsewhere in this
  paper involving ``prelogical'' dyads etc. is wrong-headed. It starts
  in the wrong place, where one agent tries to get from the world to
  ``A and B''. Instead we should start with two agents, each capable
  of making the language-entry to ``the box contains x'', who then
  jointly institute ``and''.]

A representationalist strategy starts with the world and explains our
concepts by showing how our language represents the world. A pragmatic
strategy starts with rational actors (be they people, machines, or
martians), and explains our concepts by showing how they are rooted in
proprieties of practice. (Thorough-going pragmatists extend this
strategy even to concepts that might seem to be obviously \textit{in}
the world and independent of us, such as causality. They argue that
while causality may be a feature of the world that is independent of
us, our concept of causality can be explained pragmatically, without
depending on causality-in-the world, by showing how our uses of the
concept contribute to the success of our interactions with the world.)

As this suggests, evolutionary thinking plays a critical role in
pragmatic approaches. The relation of our language practices and the
world can be explained in evolutionary terms: the reason we like to
think that our concepts represent facts in the world is that the
practices by which we institute such concepts enable us to interact
with the world in ways that are wildly successful. In other worlds,
the success of our enterprises is not due to the correctness of our
representings of the world. It' just the opposite: the correctness of
our representings is due to the successs of our enterprises. Its
because our language (i.e. our normative linguistic practice) enables
us to ``manage'' the world so successfully that we are lead to the
notion that they correctly represent the way things are.

Under an expressivist (and inferentialist) regime, \(\ulcorner A ;
B\urcorner\) \textit{expresses} (but does not represent) prelogical
inferences. The content expressed by the propositions is just the
inferential roles they play in practice; the conjunction expressed in
turn the inferential role played by \(\ulcorner ; \urcorner\). That
is, it expresses the kind of inferences we treat as correct in going
from A and B separately to A and B together and back.

Representation plays no substantial, primitive role under
expressivism.

\subsection{Propositions and Inferences}

NB: ing/ed distinction. Proposition: a proposing v. what-is-proposed.
When we assert a proposition, we assert what-is-proposed proposed.
Inference is different. An inference is a move one makes (an
inferring). But what-is-inferred when one makes an inference is not an
``inference'' but the conclusion of the inference. Still, we can reify
``inference'': it can be the action of inferring, or it can be reified
as the transition itself that one makes in inferring. Just as we can
think of ``transition'' as transitting or as the path that is
transitted. So what does it mean to assert an inference? What is it
that we thereby assert? If it's a proposition, what is it?

Assert a proposition, endorse an inference.

We can assert propositions, but we cannot assert inferences. An
inference is not the kind of thing that can be asserted. We can only
\textit{make} inferences.

To assert that a proposition is true is to assert the proposition. The
locution ``...is true'' adds nothing. And to assert a proposition is
to (publicly) express a commitment to it, and implicitly claim
entitlement to that commitment (justification).

To assert that an inference is valid is \textit{not} to assert the
inference; ``... is valid'' adds something. So instead of assertion we
use the concept of \textit{endorsement}: to \textit{endorse} an
inference is express a kind of conditional entitlement, that the
inference entitles one to a commitment to the conclusion, provided
commitment to the premises. To assert X is valid (where X is an
inference) is to endorse the inference (vouch for its validity).

Is \(P\linfer Q\) a proposition? A logical proposition? It looks like
the answer must be ``no'' in both cases. What it expresses is an
inference, and an inference is not a proposition. Remember that ``P is
true'' (where P is a proposition) expresses (and asserts) the
proposition P, but ``X is good'' (where X is an inference) does not
express X as a proposition.

But wait. We can gloss \(P\linfer Q\) as ``\(P\) entails \(Q\)'', which
is a proposition, and it makes sense to say that it is true. What's
the difference between it and \(P\rightarrow Q\)? That the latter is a
conditional and the former is unconditional? \(P\linfer Q\) expresses
as actual inference, whereas \(P\rightarrow Q\) expresses an inference
license. That's not very convincing.

And if we insist that \(P\rightarrow Q\) expresses the material
inference from P to Q, and that \(P\linfer Q\) expresses that material
inference? We do not want to say they are equal, surely. They must be
different ``modes'' of expression?

The essential difference must be that \(P\rightarrow Q\) does not in
fact express an inference; it must rather express something else, such
as goodness of inference, or license to infer, or endorsement of the
inference, or something. That \(P\linfer Q\) expresses inference OTOH
seems unproblematic.

The solution: \textbf{implication expresses not inference, but
  goodness of inference}. To assert \(P\rightarrow Q\) is to assert
not just that it is true, but that the inference \PinfQ is good.
So the \textbf{logic-entry inference from material inference to
  material implication is justified not just by the inference but by
  its goodness} ``preserves'' validity by converting it to truth: if
\(P\linfer Q\) is valid, then \(P\rightarrow Q\) is true. Asserting
\(P\rightarrow Q\) converts its truth back to validity of \(P\linfer
Q\).

(This looks like a restatement of the Deduction Theorem, but since it
precedes the formal statement of the theorem, it's a justification
rather than a restatement of it.)

Then we also need the idea that \(P\linfer Q\) expresses not just the
inference but its goodness? Which would suggest that \(\linfer\)
expresses not mere inference but good inference. What makes a material
inference good is at base a matter of normative practice. So formally
``valid inference'' is redundant, but prelogically it is not, since we
can make inferences that deviate from norms. I.e. ``prelogical
inference'' does not automatically mean valid. Same for ``material
inference'', since we can mistakes and misunderstand concepts.

later: goodness of material inference => validity of logical inference

If \(P\rightarrow Q\) is just another way of saying \(P\linfer Q\),
then why bother? We cannot express anything with it that we cannot
express without it, so what is the point of introducing it? I think
the answer is practical: we don't need it, but we want it because it
allows us to \textit{say} things that we could not \textit{say}
without it. Or rather, it allows us to say things more concisely. In
particular, it allows us to endorse the validity of \PinfQ by
asserting that \PimplQ is true.

(The facile answer is that we already have a complete logical language
that includes \(\rightarrow\), so we need to account for it. But that
begs the question. If we did not already have such a language, why
would we introduce \(\rightarrow\) once we've introduced \(\linfer\)?
What problems does it solve?)

We could elaborate a different logical language that includes
\(\linfer\) but excludes \(\rightarrow\). What would that look like?
Presumably \(\rightarrow\) would make it easier to express some
things.

Compare ``...is true''. Taking an example from Brandom, we can say
``Everything the policeman said is true.'' Without ``...is true'', we
could not do this; to express the same thing, we would have to repeat
(and reassert) everything the policeman said.

By adding \(\rightarrow\) we make it possible to express \PinfQ as a
single \textit{logical} proposition. Then we can say that \PimplQ is
true (which means that \PinfQ is valid). This also allows us to use it with
propositional variables; for example, A could stand for a proposition
that uses any number of \(\rightarrow\) instances; then we can say
simple ``A is true''. We cannot do this with \(\linfer\), since
expressions like \(P\linfer Q\) are not (logical) propositions. (They
are, however, meta-logical propositions). That's just due to the way
we have designed our logical language(s).

Furthermore: we have an introduction rule for \(\rightarrow\) but not
for \(\linfer\). So we can introduce \(\rightarrow\) by starting with
other logical propositions be we cannot do that with \(\linfer\), since
it is not a logical operator. [TODO: flesh this out]

\subsection{Notation}
\begin{itemize}
\item \(p_1\eqdef\) Pittsburgh is west of Princeton.
\item \(p_2\eqdef\) Princeton is east of Pittsburgh.
\item ``the PP inference[implication]'' - the material
  inference[implication] from \(p_1\) to \(p_2\).
\end{itemize}

\subsection{Evolution}

With this expressivist explanation of the emergence of prelogical
language in hand, we can venture a guess as to \textit{why} such
locutions should have emerged: they confer an evolutionary advantage
on the linguistic communities in which they evolved. They can say
things that communities lacking them cannot say, and they can
communicate more efficiently. It takes a lot less energy and time to
say ``Everything the policeman said is true'' than to repeat and
reassert everything the policeman said. ``He has a dog and a cat that
are cute'' is more concise than ``He has a dog that is cute, he has a
cat that is cute.''

\subsection{Bridging the gap between the non-logical and the logical}

[TODO: brief account of language entries and exits (Sellars)]

Brandom's claim is that the material implication from p1 to p2
(symbolically \(p_1\rightarrow p_2\)) expresses the material inference
from p1 to p2. If the material inference is good, then the material
implication is true, and vice-versa.

Brandom's goal is to explain logical expressivism, so he does not go
into the details of how we get from material inference to material
implication. For his purposes it is sufficient to convey the idea that
the fundamental role of logical connective \(\rightarrow\) is to
express the prelogical notion of material inference. Once this
expressive account of the logical connectives is in place, we can use
our new logical vocabulary to make logical claims about all sorts of
things.  But we cannot yet make such claims about logic itself. In
particular, we cannot offer a formal/logical account of the
institution of the logical vocabulary itself. Brandom's abbreviated
account is enthymemic: it is lacks the formal machinery needed to make
\textit{itself} explicit.

To make it explicit, we start with the following observations:

\begin{itemize}
\item Material inferences partially fix the conceptual content of
  propositions and terms (e.g. the PP inference partially fixes the
  meanings of ``east'' and ``west''.) That's what motivates the term
  ``material''.
\item Material inference goes from non-logical proposition to
  non-logical proposition, and is itself non-logical.
\item A material implication is a proposition \(p1\rightarrow p2\);
\item A proposition containing a logical connective is a logical proposition;
\item The material implication connective \(\rightarrow\) does not
  \textit{directly} express material inference. It cannot, because the
  latter involves two propositions and a transition from one to the
  other, whereas implication expresses a single proposition. What's
  missing is an explicit account of how we can get from two to one, so
  to speak.
\item The transition from material inference to material implication
  is itself a kind of inference.
  Brandom does not offer an explanation of this kind of inference - a
  way to ``make it explicit''. It's an inference that remains implicit
  in Brandom's account.
\item The inference from material inference to (material implication)
  is a \textit{logic entry} transition; it moves from non- (or
  pre-)logical inference to logical proposition.
\item The logic-entry inference, from material inference to material
  implication, cannot itself be a material inference. Although it
  might make sense to think of it as a kind of meta-material
  inference, insofar as it fixes the concepts ``material inference''
  and ``material implication'', just as our PP inference fixes the
  concepts ``east'' and ``west''. The obvious difference is that
  ``material implication'' is a logical connective and ``material
  inference'' is prelogical. So even if it is a material inference, it
  is is a different kind, since it crosses the boundary between the
  logical and the non-logical, whereas the PP material inference does
  not cross such a boundary.
\end{itemize}

To make this logic-entry inference - the transition from material
inference to logical material implication - explicit, we need a way to
make its premise explicit; that is, we need a way of directly
expressing material inference, a way that furthermore does not rely on
inference. Fortunately this is straightforward. We can use the
traditional symbol \(\linfer\) as follows:

\[\ulcorner p1\linfer p2\urcorner\ \text{expresses
material inference from}\ p_1 \text{to}\ p_2\].

Is \(p_1\linfer p_2\) a proposition? A logical proposition? It looks like
the answer must be ``no'' in both cases. What it expresses is an
inference, and an inference is not a proposition. Remember that ``P is
true'' (where P is a proposition) expresses (and asserts) the
proposition P, but ``X is good'' (where X is an inference) does not
express X as a proposition.

OTOH, if \(\ulcorner p1\linfer p2\urcorner\) is not a proposition, then
how can it have an inferential role? Traditionally the premises and
conclusion of an inference must be propositions. But modern logic
supports reasoning from inference to inference; that's what sequent
calculi do. BTW, this is a potential issue with Brandom, since he
insists that propositions and assertions are primitive, and he does
not (to my knowledge) have anything to say about reasoning with
inferences as premises and conclusions.

Entry/exit rules are like functors.

\subsection{Logic Entries}

Sequents express logic entry transitions, from informal, pre-logical
statements to formal, logical propositions.

The protasis is a combination of \textit{statements}. Statements are
the informal counterpart of formal, logical \textit{propositions}.

For example, there are many ways to declare that snow is white. Here
are a few:

\begin{itemize}
\item Snow is white.
\item Snow is \textit{definitely} white.
\item I hereby declare, categorically and without reservation, that
  snow -- yes, snow! -- is white.
\item The wet stuff that falls out of the sky when the temperature is
  below freezing is white.
\end{itemize}

For the purposes of reasoning in a formal logic system, we treat all
such statements as equivalently expressing a single
\textit{proposition}.

Logic systems generally do not capture this concept - I have not come
across a single one that does. In the sequent calculus, for example,
the same set of meta-variables is used in both the protasis and the
apodosis, as in:
\[\prop{A}, \prop{B}\linfer \prop{A}\lkand \prop{B}\]

But we can easily make the distinction explicit, unobtrusively, by
using different font faces for statements and propositions. For
statement meta-variables, we will use \(\stmt{A}, \stmt{B}, \ldots\),
and for proposition meta-variables, \(\prop{A}, \prop{B}, \ldots\). This gives us
\[\stmt{A}, \stmt{B}\linfer \prop{A}\lkand \prop{B}\]

This form makes it clear that such sequents express two kinds of logic
entry. We go from informal statements to the logical propositions that
express their ``core'' meaning, and we go from an informal combination
of statements -- \(\stmt{A}\) and also (or together with, or something
like that) \(\stmt{B}\) -- to the logical proposition that is the
conjunction of two logical propositions.

There is one problem with this, however. It will often be the case
that we want logical propositions in the protasis. For example, the
endsequent of the rule governing use of \(\lkand\) is \(\ulcorner A\land
B,\Gamma\linfer \Delta\urcorner\).

So we're forced to allow the protasis to contain statemtents and
propositions, which means that sequents will not always express logic
entries.

Well even better, we can interpret that as a logic exit, maybe? We
only have a problem if logical propositions occur on both sides of the
inference symbol. In that case, the sequent would express
intra-logical inference, and that's fine too.

So the only problem would be a protasis that contains both statements
and propositions, like \(\Gamma,\stmt{A},\prop{B}\linfer\prop{C}\), or
an apodosis that contains statements. But the former would be ok; we
would still have an entry transition \(\stmt{A}\linfer\prop{A}\), but
it would be implicit unless \(\prop{A}\) occurred on the RHS too.

\subsection{Generalization}

So far we've only addressed \textit{particular} propositions and
inferences.  The PP inference is not a general rule.

Generalizing: logic-entries go from goodness of material inferences to
truth of logical propositions.

To fully detail this we would have to address canonical v.
non-canonical inference.

Tentative: a logic entry inference rule is one that has no logical
connectives in its premises, and an exit inference rule is one that
has no logical connectives in its conclusion.


