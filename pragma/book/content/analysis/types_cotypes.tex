\chapter{Types and Cotypes}\label{ch:types}



Types as literally prototypes; cotypes as templates.

Prototypes are a starting point from which you go forward; templates are the final blueprint ...?

\paragraph{Duality}  Duality is an all-or-nothing affair. If you reverse one arrow, you must reverse all other arrows related to the first.

Formally, duality is expressed by simply reversing the arrows:
\(A\rightarrow B\) is the dual of \(A\leftarrow B\), and vice-versa.
But there is much more to the duality of types and cotypes than this.
It also involves semantic reversals that must be explicated in order
to render the duality fully intelligible. This in turn will make
evident the fully articulated inferential semantics of the concepts
``type'' and ``cotype''.

We use a regimented vocabulary in order to emphasize dualities; we
always use the prefix \textit{co} the express a dual. The drawback of
this practice is that the meanings of such \textit{co}-terms may not
be simple, direct variants of the original term. For example,
codefinition is not merely a variant kind of definition; rather, the
intended meaning of the term is \textit{underdetermines}, for reasons
that will soon become clear. Therefore reader is hereby forewarned
that the concept expressed by \textit{co-x} may not be directly
inferrable from the concept expressed by \textit{x}.

\paragraph{Notation}

\begin{description}
\item[Duality:] \dual
\item[Type and cotype:]\hspace{2em}\(A\dual \cotype{A}\)
\item[Token and cotoken:] \(\tj{a}{A}\dual
  \tj{\cotok{a}}{\cotype{A}}\)
    \item[Constructor and co-constructor:] \(\ctor{i}\dual \coctor{i}\)
    \item[Definition and codefinition:]

      \begin{align}
       \ctor{0}\times\ldots\times\ctor{n} & \rightarrow A \nonumber \\
       \coctor{0}\times\ldots\times\coctor{n} & \leftarrow \cotype{A} \nonumber
      \end{align}
      \item[List and colist:] \(\List{X}\) \dual \(\colist{X}\)
\end{description}

NB: instead of ``token'' and ``cotoken'' most writers use ``term''
(and ``coterm'') for syntax, and ``witness'', ``inhabitant'' etc. for
semantics.

%%%%%%%%%%%%%%%%
\section{Semantics}

Inferential semantics: upstream and downstream inferences determine
conceptual content.

Ctors and coctors as morphisms. Type ctors are promorphism, they go to
the type. Cotype co-ctors are apomorphisms, they go from the type.

Types are explained by upstream inferences (circumstances of
application, production rules); cotypes, by downstream inferences
(consequences, usage rules).

\paragraph{Determinateness}

A token/cotoken is fully determined, or determinate, when it is both typed and defined.

A token/cotoken is underdetermined if it lacks a type or a definition.

A token/cotoken that has a type but not a definition is
underdetermined, or partly determined.

A symbol with neither type nor definition is indeterminate.

It follows that all tokens in a type universe are at least partly
determined.

What about a null token, like the empty list? Determinate?

What is determinateness for types?

A type/cotype is fully determined/determinate if all of its tokens are
determinate.

Intuitively, a type should be underdetermined if some (at least one?)
of its tokens are underdetermined.

Indeterminate type?

What about an empty type. Determinate or indeterminate?

\subsection{Classic v. Intuitionistic Interpretation}

For the Platonist coinductive reasoning etc. presents some problems.
Cotypes are assumed to already be populated etc.  As described elsewhere in this document.

None of that will for the intuitionist. Co-constructors operate on
co-data which are anti-constructive: do not and cannot exist. So how
can an intuitionist make sense of this co-stuff?

Same way they make sense of other infinitudes? E.g. real numbers, the
continum, etc. They've figured out ways to finesse such stuff. E.g.
fans, bar theorem, apartness, etc.

\section{Types}

\subsection{Definition and codefinition}

\textquote[\cite{pierce2004advanced} ``Type Definitions'' p.
  348]{[T]he addition of definitional mechanisms can change the
  properties of type systems in ways that are not immediately
  obvious... It is therefore useful to study type definitions as
  primitive concepts.}


[Here the arrows should be read semantically, not algebraically. These
  are not functors, yet. Use a different arrow?]

Types \textit{are determined by} their constructors: \(\ctor{0}\times\ldots\times\ctor{n} \rightarrow A\).

Cotypes, by contrast, \textit{codetermine} their co-constructors:
\[\coctor{0}\times\ldots\times\coctor{n} \leftarrow \cotype{A}\]
Equivalently, and perhaps more scandalously, co-constructors are
determined by their cotype.

[TODO: compare algebraic interpretation. It lacks the idea that these
  things are related by a ``determinator'' relation.]

This may seem counter-intuitive at first glance. According to the
common way of speaking, what we call cotypes are ``defined by
coinduction'', and what we call co-constructors (often called
``destructors'') are what do the defining. But how can something we
are defining be said to determine the instruments that do the
defining? How can the \textit{definiendum} determine the
\textit{definiens}?

The beginning of wisdom here is to notice that coinduction does
\textit{not} define anything; that's one reason we use the term
``codefinition''. Codefinition always presupposes that the
\textit{(~co-~)definiendum} is antecedently available; that is, we are
to assume that cotypes and their cotokens are already ``defined'' in
some dark inscrutable way.

Fastidious anti-realists may be dissatisfied with this kind of
presupposition. For them, counter-factuals provide a way out. We can
think of a codefinition as expressing something like ``\textit{if}
this cotype were to exist, \textit{then} it would behave like so''.
Here a caveat is in order. From this perspective, codefinition may
look like a kind of contingent definition. But even if it is
contingent, it still would not count as a kind of definition. The key
word in our counter-factual is \textit{behave}. We did not say ``if it
were to exist, it would have such and such an internal structure''.
That is not how codefinition works. It says what the definiendum's
behavior must be, but does not specify the mechanism that enables the
behavior. It does not \textit{define} the cotype, it only
\textit{specifies} its behavior.

However you slice it, what comes out is that in codefinition we start
with an antecedently available notion of what the definiendum is, or
how it behaves, and from that we infer the shape of its
co-constructors. For example, to codefine streams (infinite lists), we
start with our intuitive notion of what an infinite stream would be
like, and from that we derive the notions ``head of stream'' and ``tail
of stream''. In that sense, the concept ``stream'' determines the
concepts ``head'' and ``tail''. Which makes sense: clearly it would be
rather difficult to come up with the concept ``head'' if we did not
already have an answer to ``head of what?''. It would be a little
like conceiving of the inside of a square without also coming up with
the notion of outside of a square.

With definitions, by contrast, we start out with our constructor
concepts. To define a type of finite lists of type \(X\), we start by
thinking of an empty list and a concept of combining \(X\) tokens --
which we can do even if we lack the concept ``list of \(X\)'' -- and
from that we derive the concept of a list. This is made clear by a
simple metaphor: start with a clear space on the ground and a pile of
bricks. If you can imagine stacking the bricks one upon the other, one
at a time, then you can arrive at the concept ``stack of bricks''.

Incidentally, another duality should be clear here: for finite lists,
the composition operation (``cons'') is primitive, and the
complementary operations, \head{} and \tail{}, are derivative, to be
expressed as functions. Dually, for colists, \cohead{} and \cotail{}
are the primitive operations, and \cocons{} is derived.

So cotypes ``codetermine'' their co-constructors. What exactly does
that mean? It means precisely that they fix their type signatures.
That leaves them undefined but not entirely undetermined; rather, they
are \textit{under}determined. They're partly determined, since their
types are fixed, and partly undetermined, since they are undefined. So
we use ``codetermined'' to mean ``partly determined by a fixed
type''.

Co-constructors, in turn, serve to codetermine the cotokens of the
cotype. Now this seems circular. We said that codefinition starts out
by assuming we already have the cotype; now we seem to be saying that
co-constructors, having been codetermined by their cotype, turn around
and codefine that very cotype. How does that work?

There's no paradox here if we remember that codefinition does not mean
definition. Just as cotypes codetermine their co-constructors by
fixing their types, co-constructors return the favor by expressing
constraints on the ``behavior'' not of cotypes but of cotokens. Let's spell this out more explicitly with an example:

\begin{itemize}
\item The cotype \(\colist{X}\) (stream of \(X\)) determines the type of
  its \cohead{} co-constructor:
  \(\tj{\cotok{x}}{\colist{X}}\linfer\tj{\cohead(\cotok{x})}{X}\).
  \item The co-constructor in turn expresses a constraint on cotokens
    of the cotype: their \cohead{} must be of type \(X\).
\end{itemize}

And here again ``codefine'' means ``partly determine''. The
co-constructor fixes the type of its output but does not define it.
That leaves it underdetermined,, and it will remain underdetermined
until the co-constructor becomes fully determined by a cofunction
definition, to which we now turn.

Critically important: the partial determinations expressed by the
co-constructors are global! They apply to every cotoken of the cotype.

\subsection{Constructor and Co-constructor}

Ctors and co-ctors determine (define?) a protocol. A functor tin CT
terms. Cofunctions implement the protocol. Each has its own
definitions for the co-ctors. So there's a naming issue in practice.
If they all use the same name the we have name clashes.

A way around this is to have each cofunction determine a type. Then
the compiler can determine which co-ctor implementation is needed
based on the type.

The other way would be for each cofunction to define a unique name for
the co-ctors. But then they would not be co-ctors for the type.

Either way: do cotypes and cofunctions make sense without some kind of
polymorphism?

\subsection{Function and Cofunction}

Domain and codomain.

At the cotype level, the co-constructors express type constraints on
\textit{all} cotokens of the co-type.

Cofunctions refine the co-constructors by defining them. But this
refinement is \textit\textit{local}! It only applies to cotokens
produced by the cofunction.

We can think of the co-constructors as expressing a generic or global
interface or signature for cotokens of the cotype. Then cofunctions
provide implementations\footnote{Co-implementations?}.

(Compare: inheritance)

\subsection{Note}

Some writers seem to get it wrong, unless I've badly misunderstood something.

For example:

\textquote[\cite{nonwf_deduction}, p. 7]{A coinductively defined set
  is also constructed by starting with a set of initial elements and
  applying the constructor operators, possibly infinitely many times.
  One example, which arises from the same initial element and
  constructors as the inductive set of lists, is that of possibly
  infinite lists, i.e. the set that also contains infinite streams.
  The fact that we can apply the operators infinitely many times is
  due to coinduction being the largest subset that can (potentially)
  be constructed using the operators.} That makes no sense at all to
me. The number of times you iterate a ctor in building a single
element has nothing to do with the size of the set. Worse, what this
writer has evidently completely missed is that coinduction does not
construct things. The infinitely long lists in a stream type are
\textit{given}, not constructed! Which is odd, considering how
technical the paper is, he must know a thing or two.

Even if we did construct some stuff, the original type would always be
at least as big, just because it contains the most stuff possible.
This is evident when you consider the co-ctors are apomorphisms. You
cannot add a new, or longer stream term to a stream type! So any
cofunctions you define will determine a set that is always less than
or equal to the original stream set.


%%%%%%%%%%%%%%%%
\section{Cotypes}

Cotypes are specified, not defined. The specification tells us how to
recognize cotype instances when we see them, but it does not tell us
how to build them.

\paragraph{Case study: function-oriented streams}

The following diagram, from \cite{jacobs_intro_coalgebra}, is intended
to depict the coalgebra of infinite lists. Here \(\exp{A}{\Nat}\) is
the set of all functions from domain \(\Nat\) to arbitrary set \(A\) -
note that these are sets, not types. The symbol \(\sigma\) is an
arbitrary element of \(\exp{A}{\Nat}\):

\begin{tikzpicture}
  \node at (0,0) (a) {$A^{\Nat}$};
  \node at (2.5in,0) (b) {$A\times{}A^{\Nat}$};
  \draw[-Latex] (a) -- (b) node[pos = .5, above]
       {\scriptsize\(\sigma\mapsto (\sigma(0), \lambda{}n.\sigma(n+1))\)};
\end{tikzpicture}%

It looks simple, but in fact it is very complex.

To pull it apart, we start by assigning some names and making the type
structure explicit. First consider the maplet:
\begin{align}
  & \sigma\mapsto (\sigma(0), \lambda{}n.\sigma(n+1))\label{sigmap}
\end{align}

\begin{itemize}
\item The symbol \(\mapsto\) obviously represents a function, so let's
  name it. It takes a single argument and uses it to produce a pair, so
  we'll call it \texttt{pairgen}. Note that since its argument is a
  function, \texttt{pairgen} is a higher-order function; in particular
  it is \textit{not} a member of \(\exp{A}{\Nat}\). We can declare its
  type and define it schematically:
  \begin{align*}
    & \tj{\texttt{pairgen}}{\exp{A}{\Nat}\fnarrow(A\times\exp{A}{\Nat})} \\
    & \texttt{pairgen}\eqdef\lambda{}f.(f(0),\lambda{}n.f(n+1)) & \textit{untyped} \\
    & \texttt{pairgen}\eqdef\lambda{}f:\exp{A}{\Nat}.(f(0),\lambda{}n.f(n+1)):A\times\exp{A}{\Nat} & \textit{typed}
  \end{align*}
\item The pairing uses standard \((\, ,\, )\) syntax to denote the pairing
  operation. To make this explicit, we'll call it \texttt{pair}. This
  is the primitive binary pairing operation that does nothing more
  than combine its arguments without using them.
\item The second element of the output pair is a lambda expression, so
  let's name that too:
  \begin{equation}
  g\eqdef\lambda{}n.f(n+1) \label{gdef}
  \end{equation}
  But in this expression the argument to \(f\) is itself a function
  application, so we can abstract out the function:
  \(h\eqdef\lambda{}m.m+1\).
\item The function we've just defined, \(h\) uses two constant
  symbols, \(+\) and \(1\). We can parameterize them:
  \(h'\eqdef\lambda{}c,op,m.op(m,c)\). Then we can rewrite (\ref{gdef}) as:
  \begin{equation}
    g\eqdef\lambda{}n.f(h'(1,+,n))
  \end{equation}
  \item Alternatively, notice that \(h\eqdef\lambda{}m.m+1\) is a
    member of \(\exp{\Nat}{\Nat}\) - an injective endomorphism, to be
    precise. In fact we can replace it with any \textit{arbitrary}
    injective member of \(\exp{\Nat}{\Nat}\)! So we can rewrite
    \ref{gdef} as simply
    \begin{align}
      & g\eqdef\lambda{}n.f(h(n))\hfill &
      \text{where}\ \tj{h}{\exp{\Nat}{\Nat}}, h\ \text{injective}
    \end{align}
  \item We're now in a position to generalize \ref{sigmap}:
    \begin{align}
      & \sigma\mapsto (\sigma(0), \lambda{}n.\sigma(n+1)) \\
      & \texttt{pairgen}\eqdef\lambda{}f.(f(0),g(n))
    \end{align}
    Which means that \ref{sigmap} will work with \texttt{pairgen}
    parameterized by \textit{any} injective member of
    \(\exp{\Nat}{\Nat}\). If we were to iterate over those, we would
    produce one ``permutation'' per function.
\end{itemize}

The upshot of all this is that what we've described using
\(\exp{A}{\Nat}\) is not \textit{the} cotype of infinite streams, but
a family of instances indexed by the members \(\exp{\Nat}{\Nat}\) --
each a ``proof'' of the (meta-)type (proofs of the stream type or just
streams). In other words, if we can think of a stream as generated by
a function over \(\Nat\), we can think of the \texttt{tail} operation
as a family of ... or something.

Think of these not as ``proofs'' of the type, but as refinements.
They're subtypes, not members. But there is a sense in which
refinements count as proofs, I think.

We prove \(\Nat\) by constructing a natural number. But if we cannot
construct an element of a cotype, then what counts as proof of the
type? Answer: an implementation that conforms to the spec. So the sets
we've just described are all equally ``the'' set of streams, but they
are all different ``implementations'', which also means they are
\textit{intensionally} different. Which would imply intensional
semantics make a real difference for cotypes. Also that proofs of
cotypes are intensional.

%%%%%%%%%%%%%%%%
\section{Morphism classes}\label{morph:classes}

FIXME: move to taxonomy

First some terminology. We classify morphisms for a type/cotype by the
role played by the type in the type signature of the morphism. For
example, if the type we're interested in is the domain of \(f\), then
\(f\) is an apomorphism.

If the type (or cotype) of interest is \(X\), then:

\begin{description}
\item[Endomorphism\footnotemark:]\footnotetext{Greek ἕνδον,
  \textit{endon}, inside, within, internal} \(\tj{f}{\morph{X}{X}}\)
\item[Promorphism\footnotemark:]\footnotetext{Greek προσ, \textit{pros},
  to, toward} \(\tj{f}{\morph{A}{X}}\)
\item[Apomorphism\footnotemark:]\footnotetext{Greek απο, \textit{apo},
  from, away from} \(\tj{f}{\morph{X}{A}}\)
\item[Synthetic morphism:] the codomain is a polynomial type
  (combination of products and sums) containing \(X\) e.g.
  \begin{itemize}
  \item \(\tj{f}{\morph{X}{X}\times X}\) is a synthetic endomorphism
  \item \(\tj{f}{\morph{X}{A}\times B}\) is a synthetic apomorphism
  \end{itemize}
\item[Analytic morphism:] the domain is a polynomial type containing \(X\), e.g.
  \begin{itemize}
    \item \(\tj{f}{X\times \morph{X}{X}}\) is an analytic endomorphism
    \item \(\tj{f}{X\times \morph{X}{A}}\) is an analytic apomorphism
  \end{itemize}
\item[Hybrid morphism:] anything that does not fit in the above categories:
  \begin{itemize}
  \item \(\tj{f}{\morph{X}{A}\times X}\)
  \item \(\tj{f}{\morph{X}{X}\times B}\)
  \item \(\tj{f}{\morph{X}{A}\times B + C}\)
  \item \(\tj{f}{A\times \morph{X}{X}}\)
  \item \(\tj{f}{A\times \morph{X}{B}}\)
  \item \(\tj{f}{\morph{X}{A}\times B + C}\)
  \item etc.
  \end{itemize}
\end{description}


%%%%%%%%%%%%%%%%
\section{Implementation}

An implementation is an approximation.

Example: constant lists, whose heads always return a constant c.

How do we get the ball rolling? We need a cofunction whose head
returns c. But it must be applied to a stream token; where do we get
one, if they do not exist?

Cotokens are underdetermined. We can approximate one in a real program
by just ignoring it. We know how it would behave if we had it, so we
can implement the behavior even if we do not have one.  So we don't just approximate the infinity, we approximate the existence.

So we implement a \defn{constantly} function that takes a number to
return. Instead of returning it we create and return a cofunction
whose head returns it. But then to use it the client would need to use
that head function.

So we're forced to implement either some kind of object-oriented
thing, or we have protocols, and our returned cofunction would
implement the protocol. That fits. The cotype co-ctors define the
protocol.


%%%%%%%%%%%%%%%%
\section{Calculi}
The concepts of types and cotypes may be presented in a variety of forms.

\section{Typed calculus of types and cotypes}

This section uses the kind of type-theoretic notation commonly used to
present types and cotypes. Derived from Martin-Löf Type Theory by way
of HoTT.

\subsection{Logic of types and cotypes}

This section presents types, cotypes, etc. using a sequent calculus
commonly used to express various logics.

\subsection{Algebraics of types and cotypes}

Yet a third mode of presentation takes an algebraic perspective. This
is a popular way of presenting and explaining induction and
coinduction.

